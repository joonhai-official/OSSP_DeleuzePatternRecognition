{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ì„¤ì¹˜ (ê°€ë²¼ìš´ ì¡°í•©: openai, gradio, plotlyë§Œ)\n",
        "!pip -q install --upgrade --force-reinstall \"httpx==0.27.2\" \"openai==1.51.0\"\n",
        "!pip -q install --upgrade \"gradio==4.44.0\" \"plotly==5.24.1\"\n",
        "\n",
        "# ì´ë¯¸ ë¡œë“œëœ ëª¨ë“ˆ ì œê±°(Colab ì”ì¡´ ëª¨ë“ˆ ì¶©ëŒ ë°©ì§€)\n",
        "import sys\n",
        "for m in [\"httpx\", \"openai\"]:\n",
        "    if m in sys.modules:\n",
        "        del sys.modules[m]\n",
        "\n",
        "import httpx, openai\n",
        "print(\"httpx:\", httpx.__version__)\n",
        "print(\"openai:\", openai.__version__)\n",
        "print(\"âœ… install ok\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wd19geR0Xv9t",
        "outputId": "80b50b11-c796-4ba1-ebf5-d09aed82838c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dataproc-spark-connect 0.8.3 requires websockets>=14.0, but you have websockets 12.0 which is incompatible.\n",
            "firebase-admin 6.9.0 requires httpx[http2]==0.28.1, but you have httpx 0.27.2 which is incompatible.\n",
            "google-adk 1.11.0 requires websockets<16.0.0,>=15.0.1, but you have websockets 12.0 which is incompatible.\n",
            "google-genai 1.30.0 requires httpx<1.0.0,>=0.28.1, but you have httpx 0.27.2 which is incompatible.\n",
            "google-genai 1.30.0 requires websockets<15.1.0,>=13.0.0, but you have websockets 12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mhttpx: 0.28.1\n",
            "openai: 1.51.0\n",
            "âœ… install ok\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ê¸°ëŠ¥ ì¤‘ì‹¬ ìœ í‹¸: ê³µí†µ ìƒìˆ˜/ê°„ë‹¨ í•¨ìˆ˜\n",
        "import os, re, time, socket, textwrap, traceback\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional, List\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# ê¸´ ì‘ë‹µì„ ì„¸ê·¸ë¨¼íŠ¸ë¡œ ì´ì–´ë¶™ì´ëŠ” íŒŒë¼ë¯¸í„°(ì‚¬ì‹¤ìƒ ê¸¸ì´ ì œí•œ ì™„í™”)\n",
        "LONG_SEGMENT_TOKENS = 1500\n",
        "LONG_MAX_SEGMENTS   = 12\n",
        "\n",
        "def md(s: str) -> str:\n",
        "    return textwrap.dedent(s).strip()\n",
        "\n",
        "def find_free_port(start=7860, end=7950) -> int:\n",
        "    for p in range(start, end+1):\n",
        "        try:\n",
        "            with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
        "                s.settimeout(0.1)\n",
        "                if s.connect_ex((\"127.0.0.1\", p)) != 0:\n",
        "                    return p\n",
        "        except:\n",
        "            pass\n",
        "    raise OSError(\"No free port\")\n"
      ],
      "metadata": {
        "id": "okEtpAbZXv7b"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ìƒì„±ì€ OpenAI APIë§Œ ì‚¬ìš©. httpx Clientë¥¼ ì§ì ‘ ì£¼ì…í•´ì„œ proxies ì¶©ëŒ ê²½ë¡œ ì°¨ë‹¨.\n",
        "import httpx\n",
        "from openai import OpenAI\n",
        "\n",
        "_HTTPX = httpx.Client(timeout=httpx.Timeout(60.0, connect=30.0, read=60.0))\n",
        "\n",
        "def _openai_client() -> OpenAI:\n",
        "    if not os.environ.get(\"OPENAI_API_KEY\"):\n",
        "        raise RuntimeError(\"OPENAI_API_KEYê°€ ì—†ìŠµë‹ˆë‹¤. ìƒë‹¨ UIì—ì„œ ì €ì¥í•˜ì„¸ìš”.\")\n",
        "    # í•µì‹¬: http_client=_HTTPX ì£¼ì…\n",
        "    return OpenAI(http_client=_HTTPX)\n",
        "\n",
        "def _openai_chat(system, user, model, temperature, top_p, max_tokens):\n",
        "    client = _openai_client()\n",
        "    resp = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[{\"role\":\"system\",\"content\":system},{\"role\":\"user\",\"content\":user}],\n",
        "        temperature=float(temperature), top_p=float(top_p), max_tokens=int(max_tokens),\n",
        "    )\n",
        "    return (resp.choices[0].message.content or \"\").strip()\n",
        "\n",
        "def llm_once(system, user, model_id, temperature, top_p, max_tokens):\n",
        "    return _openai_chat(system, user, model_id, temperature, top_p, max_tokens)\n",
        "\n",
        "def llm_long(system, user, model_id, temperature=0.7, top_p=0.9,\n",
        "             segment_tokens=LONG_SEGMENT_TOKENS, max_segments=LONG_MAX_SEGMENTS, stop_marker=\"[END]\"):\n",
        "    out = []\n",
        "    prompt = user + \"\\n\\n(ë¬¸ì„œê°€ ê¸¸ë©´ ì—¬ëŸ¬ ë²ˆ ë‚˜ëˆ  ì‘ì„±. ì™„ì „íˆ ëë‚˜ë©´ ë§ˆì§€ë§‰ ì¤„ì— [END])\"\n",
        "    for _ in range(max_segments):\n",
        "        seg = llm_once(system, prompt, model_id, temperature, top_p, segment_tokens)\n",
        "        seg = (seg or \"\").strip()\n",
        "        out.append(seg)\n",
        "        if stop_marker in seg:\n",
        "            break\n",
        "        prompt = \"ì´ì–´ì„œ ê³„ì†. ë°˜ë³µ ì—†ì´ ìì—°ìŠ¤ëŸ½ê²Œ ì—°ê²°. ëë‚˜ë©´ [END].\"\n",
        "    return \"\\n\".join(out).replace(stop_marker, \"\").strip()\n",
        "\n",
        "print(\"âœ… LLM ì—”ì§„ ì¤€ë¹„ ì™„ë£Œ (OpenAI + httpx ì£¼ì…)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BMZOWQ6UXv5T",
        "outputId": "9b11ffb7-0a39-4a96-d93b-be203363e910"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… LLM ì—”ì§„ ì¤€ë¹„ ì™„ë£Œ (OpenAI + httpx ì£¼ì…)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# A) ìˆœì • GPT â€” ë¬¸ë‹¨í˜•(ë¹„êµ ê¸°ì¤€)\n",
        "BASELINE_SYSTEM = \"\"\"You are a helpful assistant.\n",
        "Write the answer in Korean as plain paragraphs only.\n",
        "Do NOT use headings, lists, tables, math, code blocks, KPIs, or IFâ€“THEN rules.\n",
        "No hard limit on length; write comprehensively.\n",
        "\"\"\"\n",
        "BASELINE_USER_TPL = \"Question:\\n{q}\\n\\n(Write only plain paragraphs. No explicit structure.)\"\n",
        "\n",
        "# B) ë“¤ë¢°ì¦ˆ ì˜¤í¼ë ˆì´í„° â€” 6ê°œ ì„¹ì…˜ ê°•ì œ + í‘œ/ìˆ˜ì‹ í¬í•¨\n",
        "DELEUZE_SYSTEM = md(\"\"\"\n",
        "You are a Deleuzian policy designer.\n",
        "Always output ALL the following SECTIONS in Korean with the exact headings:\n",
        "\n",
        "### ë¬¸ì œ-ì´ë°ì•„\n",
        "- ëª©í‘œ, ì œì•½, ê²°ì •ë³€ìˆ˜, ë¶ˆí™•ì‹¤ì„±(íŠ¹ì´ì )ì„ 5~9ì¤„\n",
        "\n",
        "### í”„ë¡ í‹°ì–´(í‘œ)\n",
        "- ë³´ìˆ˜/ê· í˜•/ì§„ë³´ 3ëŒ€ì•ˆ í‘œ 1ê°œ (ì—´: ëŒ€ì•ˆ, í•µì‹¬ìˆ˜ë‹¨, ê¸°ëŒ€íš¨ê³¼, Trade-off, ì¬ì •ì„¤ëª…)\n",
        "- Markdown í‘œë¡œ ì‘ì„±\n",
        "\n",
        "### ëŒ€ì•ˆ ìƒì„¸(3ê°œ)\n",
        "- ê° ëŒ€ì•ˆì„ 6~10ì¤„: í•µì‹¬ ë ˆë²„ 3ê°œ, ê¸°ëŒ€íš¨ê³¼, ìœ„í—˜/ì™„í™”, ì‹¤í–‰ ë¡œë“œë§µ\n",
        "\n",
        "### ìŠ¤ìœ„ì¹­ ë£°(IFâ€“THEN)\n",
        "- 3~7ê°œ ê·œì¹™, ê° 1ì¤„. ì˜ˆ) IF PG>25% THEN H+=10 AND clawback-=5\n",
        "\n",
        "### KPI/í”¼ë“œë°±\n",
        "- KPI 3~6ê°œ(ì •ì˜+ì¸¡ì •), ë¶„ê¸°ë³„ ì¡°ì • ì˜ì‚¬ì½”ë“œ 5~9ì¤„ (```text ë¸”ë¡ ì‚¬ìš©)\n",
        "\n",
        "### ìˆ˜ì‹/ì œì•½\n",
        "- ë°˜ë“œì‹œ ```math ë¸”ë¡\n",
        "- B = E_eff + Î”VAT(Ï„_vat) + Î”Top(Ï„_top) + clawback(T) - {Cash(H) + W_sub(W) + Training + Admin}\n",
        "- ì œì•½: Bâ‰¥0, Adminâ‰¤2%, EMTRâ‰¤Ï„*\n",
        "\"\"\")\n",
        "\n",
        "DELEUZE_USER_TPL = md(\"\"\"\n",
        "ì§ˆë¬¸:\n",
        "{q}\n",
        "\n",
        "ê·¼ê±° í…ìŠ¤íŠ¸(ì„ íƒ):\n",
        "{ctx}\n",
        "\n",
        "ì§€ì‹œ:\n",
        "- ìœ„ 6ê°œ ì„¹ì…˜ì„ ë°˜ë“œì‹œ ëª¨ë‘ ì¶œë ¥í•œë‹¤.\n",
        "- í”„ë¡ í‹°ì–´ëŠ” ë§ˆí¬ë‹¤ìš´ í‘œ, ìˆ˜ì‹ì€ ```math ë¸”ë¡ìœ¼ë¡œ ì‘ì„±í•œë‹¤.\n",
        "- [END]ëŠ” ì“°ì§€ ì•ŠëŠ”ë‹¤.\n",
        "\"\"\")\n",
        "\n",
        "REQ_HEADS = [\"### ë¬¸ì œ-ì´ë°ì•„\",\"### í”„ë¡ í‹°ì–´(í‘œ)\",\"### ëŒ€ì•ˆ ìƒì„¸(3ê°œ)\",\"### ìŠ¤ìœ„ì¹­ ë£°\",\"### KPI/í”¼ë“œë°±\",\"### ìˆ˜ì‹/ì œì•½\"]\n",
        "\n",
        "def is_deleuze_structured(text: str) -> bool:\n",
        "    has_sections = all(h in text for h in REQ_HEADS)\n",
        "    has_table    = (\"|\" in text and \"---\" in text)\n",
        "    has_math     = \"```math\" in text\n",
        "    return has_sections and has_table and has_math\n",
        "\n",
        "def repair_deleuze(text, q, ctx, model, temperature, top_p):\n",
        "    if is_deleuze_structured(text):\n",
        "        return text\n",
        "    missing = [h for h in REQ_HEADS if h not in text]\n",
        "    sys = \"You are a strict editor. Fill ONLY the missing sections exactly as specified.\"\n",
        "    usr = md(f\"\"\"\n",
        "ëˆ„ë½ëœ ì„¹ì…˜: {\", \".join(missing)}\n",
        "ì§ˆë¬¸: {q}\n",
        "ê·¼ê±°: {ctx or \"ì—†ìŒ\"}\n",
        "\n",
        "[ì´ˆì•ˆ ì‹œì‘]\n",
        "{text}\n",
        "[ì´ˆì•ˆ ë]\n",
        "\n",
        "ê·œì¹™:\n",
        "- ê¸°ì¡´ í…ìŠ¤íŠ¸ëŠ” ìˆ˜ì •í•˜ì§€ ë§ê³ , ëˆ„ë½ëœ ì„¹ì…˜ë§Œ ì¶”ê°€.\n",
        "- í”„ë¡ í‹°ì–´ëŠ” ë§ˆí¬ë‹¤ìš´ í‘œ, ìˆ˜ì‹ì€ ```math ë¸”ë¡.\n",
        "\"\"\")\n",
        "    fixed = llm_long(sys, usr, model, temperature, top_p, segment_tokens=1200, max_segments=4)\n",
        "    return fixed if is_deleuze_structured(fixed) else text\n"
      ],
      "metadata": {
        "id": "cvvznLO4Xv2_"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Markdown í‘œ â†’ DataFrame\n",
        "def parse_markdown_table(md_text: str) -> Optional[pd.DataFrame]:\n",
        "    lines = [ln.rstrip() for ln in md_text.splitlines()]\n",
        "    for i, ln in enumerate(lines):\n",
        "        if \"|\" in ln and re.search(r\"\\|\\s*-+\\s*\\|\", ln):\n",
        "            header_idx = i-1 if i>0 else None\n",
        "            if header_idx is None:\n",
        "                continue\n",
        "            block = []\n",
        "            j = header_idx\n",
        "            while j < len(lines) and \"|\" in lines[j]:\n",
        "                block.append(lines[j]); j += 1\n",
        "            hdr = [c.strip() for c in block[0].strip(\"|\").split(\"|\")]\n",
        "            body = []\n",
        "            for row in block[2:]:\n",
        "                cols = [c.strip() for c in row.strip(\"|\").split(\"|\")]\n",
        "                if len(cols) == len(hdr):\n",
        "                    body.append(cols)\n",
        "            if body:\n",
        "                return pd.DataFrame(body, columns=hdr)\n",
        "    return None\n",
        "\n",
        "# ì˜µì…˜ ë¼ë²¨ í‘œì¤€í™”\n",
        "def _normalize_option_names(series: pd.Series) -> pd.Series:\n",
        "    def map_name(s: str) -> str:\n",
        "        t = str(s); tl = t.lower()\n",
        "        if any(k in t for k in [\"ë³´ìˆ˜\",\"ë³´ìˆ˜í˜•\"]) or \"conservative\" in tl: return \"ë³´ìˆ˜\"\n",
        "        if any(k in t for k in [\"ê· í˜•\",\"ì¤‘ë„\"])   or \"balanced\" in tl or \"moderate\" in tl: return \"ê· í˜•\"\n",
        "        if any(k in t for k in [\"ì§„ë³´\",\"ê³µê²©\",\"í™•ëŒ€\",\"ê°•í™”\"]) or \"progressive\" in tl or \"aggressive\" in tl: return \"ì§„ë³´\"\n",
        "        return t\n",
        "    return series.apply(map_name)\n",
        "\n",
        "# í…ìŠ¤íŠ¸ í‚¤ì›Œë“œ ê¸°ë°˜ ì ìˆ˜\n",
        "_FISCAL_POS = [\"ì„¸ìˆ˜\",\"ì„¸ì…\",\"í‘ì\",\"ì ˆê°\",\"ì§€ì¶œ íš¨ìœ¨\",\"í´ë¡œë°±\",\"ì¬ì›\",\"ê¸°ê¸ˆ\",\"ê· í˜•ì¬ì •\",\"ê±´ì „ì„±\"]\n",
        "_FISCAL_NEG = [\"ì ì\",\"ì§€ì¶œ í™•ëŒ€\",\"ë³´ì¡°ê¸ˆ\",\"ë¶€ì±„\",\"ê°ì„¸\",\"ì¬ì • ì•…í™”\",\"ì ìì „í™˜\",\"ì¬ì • ë¶€ë‹´\"]\n",
        "_EFFECT_POS = [\"íš¨ìœ¨\",\"íš¨ê³¼\",\"ì„±ê³¼\",\"ê°ì¶•\",\"ë‹¬ì„±\",\"ëª©í‘œ ê²½ë¡œ\",\"ê°€ê²©ì‹ í˜¸\",\"íƒ€ê²ŒíŒ…\",\"ì§€í‘œ\",\"ëª¨ë‹ˆí„°ë§\",\"í”¼ë“œë°±\",\"ì •í™•\"]\n",
        "_EFFECT_NEG = [\"ì˜ì¡´\",\"ì§€ì—°\",\"ë¶€ì‘ìš©\",\"ë¹„íš¨ìœ¨\",\"ì™œê³¡\",\"ê³¼ì†Œì§€ì›\",\"ì§‘í–‰ ì§€ì²´\"]\n",
        "_NUM_PAT  = re.compile(r\"(\\d+(?:\\.\\d+)?)\\s*(%|í†¤|MW|GW|ì¡°ì›|ì–µì›|ë§Œì›|ë°°|p|í¼ì„¼íŠ¸)\")\n",
        "\n",
        "def _keyword_score(text: str, pos_words: List[str], neg_words: List[str]) -> float:\n",
        "    s = str(text)\n",
        "    pos = sum(1 for w in pos_words if w in s)\n",
        "    neg = sum(1 for w in neg_words if w in s)\n",
        "    return float(pos - neg)\n",
        "\n",
        "def _magnitude_boost(text: str) -> float:\n",
        "    matches = list(_NUM_PAT.finditer(str(text)))\n",
        "    return min(5.0, 0.8 * len(matches))\n",
        "\n",
        "def _normalize_minmax(arr: np.ndarray) -> np.ndarray:\n",
        "    lo, hi = float(np.min(arr)), float(np.max(arr))\n",
        "    if hi - lo < 1e-8:\n",
        "        return np.full_like(arr, 50.0)\n",
        "    return 100.0 * (arr - lo) / (hi - lo)\n",
        "\n",
        "def frontier_to_numeric(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df2 = df.copy()\n",
        "    ren = {}\n",
        "    for c in df2.columns:\n",
        "        if \"ëŒ€ì•ˆ\" in c: ren[c] = \"ëŒ€ì•ˆ\"\n",
        "        if \"í•µì‹¬\" in c: ren[c] = \"í•µì‹¬ìˆ˜ë‹¨\"\n",
        "        if \"ê¸°ëŒ€íš¨\" in c: ren[c] = \"ê¸°ëŒ€íš¨ê³¼\"\n",
        "        if \"Trade\" in c or \"trade\" in c.lower(): ren[c] = \"Trade-off\"\n",
        "        if any(k in c for k in [\"ì˜ˆì‚°\",\"B\",\"ì¬ì •\"]): ren[c] = \"ì¬ì •ì„¤ëª…\"\n",
        "    df2.rename(columns=ren, inplace=True)\n",
        "    if \"ëŒ€ì•ˆ\" not in df2.columns:\n",
        "        df2.rename(columns={df2.columns[0]:\"ëŒ€ì•ˆ\"}, inplace=True)\n",
        "\n",
        "    df2[\"ëŒ€ì•ˆ\"] = _normalize_option_names(df2[\"ëŒ€ì•ˆ\"])\n",
        "\n",
        "    def row_text(row):\n",
        "        cols = []\n",
        "        for k in [\"ëŒ€ì•ˆ\",\"í•µì‹¬ìˆ˜ë‹¨\",\"ê¸°ëŒ€íš¨ê³¼\",\"Trade-off\",\"ì¬ì •ì„¤ëª…\"]:\n",
        "            if k in df2.columns: cols.append(str(row.get(k,\"\")))\n",
        "        return \" \".join(cols)\n",
        "\n",
        "    fiscal_raw, effect_raw = [], []\n",
        "    for _, row in df2.iterrows():\n",
        "        txt = row_text(row)\n",
        "        f = _keyword_score(txt, _FISCAL_POS, _FISCAL_NEG) + _magnitude_boost(txt)\n",
        "        e = _keyword_score(txt, _EFFECT_POS, _EFFECT_NEG) + _magnitude_boost(txt)\n",
        "        fiscal_raw.append(f); effect_raw.append(e)\n",
        "\n",
        "    fiscal_arr = _normalize_minmax(np.array(fiscal_raw, dtype=float))\n",
        "    effect_arr = _normalize_minmax(np.array(effect_raw, dtype=float))\n",
        "\n",
        "    df2[\"ì¬ì • ê¸°ì—¬ ì§€ìˆ˜(í…ìŠ¤íŠ¸ ê¸°ë°˜)\"] = np.round(fiscal_arr, 1)\n",
        "    df2[\"ì„±ê³¼ ì§€ìˆ˜(í…ìŠ¤íŠ¸ ê¸°ë°˜)\"]   = np.round(effect_arr, 1)\n",
        "\n",
        "    # íŒŒë¼ë¯¸í„° ì˜ˆì‹œ(í•µì‹¬ìˆ˜ë‹¨ í…ìŠ¤íŠ¸ ê¸¸ì´ì— ë¹„ë¡€)\n",
        "    if \"í•µì‹¬ìˆ˜ë‹¨\" in df2.columns:\n",
        "        ln = df2[\"í•µì‹¬ìˆ˜ë‹¨\"].astype(str).str.len()\n",
        "        df2[\"param_ì„¸ìœ¨/ê°€ê²©ì‹ í˜¸\"]   = (ln/100).clip(0, 1.0).round(2)\n",
        "        df2[\"param_ì •ë°€í™˜ê¸‰/í´ë¡œë°±\"] = (ln/140).clip(0, 1.2).round(2)\n",
        "        df2[\"param_í›ˆë ¨/ì „í™˜ì§€ì›\"]   = (ln/60).clip(0, 6).round(2)\n",
        "    else:\n",
        "        n = len(df2)\n",
        "        df2[\"param_ì„¸ìœ¨/ê°€ê²©ì‹ í˜¸\"]   = np.linspace(0.2, 0.8, n).round(2)\n",
        "        df2[\"param_ì •ë°€í™˜ê¸‰/í´ë¡œë°±\"] = np.linspace(0.3, 0.9, n).round(2)\n",
        "        df2[\"param_í›ˆë ¨/ì „í™˜ì§€ì›\"]   = np.linspace(2, 6,   n).round(2)\n",
        "\n",
        "    # ì¢…í•©ì ìˆ˜(ë­í‚¹ìš©): ì„±ê³¼(0.6)+ì¬ì •(0.4)\n",
        "    df2[\"ì¢…í•©ì ìˆ˜\"] = np.round(0.6*df2[\"ì„±ê³¼ ì§€ìˆ˜(í…ìŠ¤íŠ¸ ê¸°ë°˜)\"] + 0.4*df2[\"ì¬ì • ê¸°ì—¬ ì§€ìˆ˜(í…ìŠ¤íŠ¸ ê¸°ë°˜)\"], 1)\n",
        "    return df2\n",
        "\n",
        "def build_plots(df_num: pd.DataFrame):\n",
        "    xname, yname = \"ì„±ê³¼ ì§€ìˆ˜(í…ìŠ¤íŠ¸ ê¸°ë°˜)\", \"ì¬ì • ê¸°ì—¬ ì§€ìˆ˜(í…ìŠ¤íŠ¸ ê¸°ë°˜)\"\n",
        "\n",
        "    fig1 = go.Figure()\n",
        "    fig1.add_trace(go.Scatter(\n",
        "        x=df_num[xname], y=df_num[yname],\n",
        "        mode=\"markers+text\",\n",
        "        text=df_num[\"ëŒ€ì•ˆ\"].astype(str),\n",
        "        textposition=\"top center\",\n",
        "        name=\"ì •ì±…ì•ˆ\"\n",
        "    ))\n",
        "    fig1.update_layout(\n",
        "        title=\"í”„ë¡ í‹°ì–´ â€” ì„±ê³¼ vs ì¬ì •(í‘œ í…ìŠ¤íŠ¸ ê·¼ê±° ì •ê·œí™”)\",\n",
        "        xaxis_title=f\"{xname} (â†‘ì¢‹ìŒ)\", yaxis_title=f\"{yname} (â†‘ì¬ì •ê±´ì „ì„± ê¸°ì—¬)\",\n",
        "        template=\"plotly_white\", height=360\n",
        "    )\n",
        "\n",
        "    fig2 = go.Figure()\n",
        "    param_cols = [c for c in df_num.columns if c.startswith(\"param_\")]\n",
        "    for col in param_cols:\n",
        "        fig2.add_trace(go.Bar(x=df_num[\"ëŒ€ì•ˆ\"], y=df_num[col], name=col.replace(\"param_\",\"\")))\n",
        "    fig2.update_layout(title=\"í•µì‹¬ íŒŒë¼ë¯¸í„° ë¹„êµ(í…ìŠ¤íŠ¸ ìœ ë„)\", barmode=\"group\",\n",
        "                       template=\"plotly_white\", height=360)\n",
        "\n",
        "    t = np.arange(1, 7)\n",
        "    base_eff = float(np.nanmean(df_num[xname].values)) if len(df_num) else 50.0\n",
        "    base_fsc = float(np.nanmean(df_num[yname].values)) if len(df_num) else 50.0\n",
        "    pg = (28 - 0.10*(base_eff-50) - 1.0*(t-1) + np.random.randn(len(t))*0.25).clip(12,30)\n",
        "    B  = (-0.2 + 0.02*(base_fsc-50) + 0.4*np.sin(t/1.2) + np.random.randn(len(t))*0.1).round(2)\n",
        "\n",
        "    fig3 = go.Figure()\n",
        "    fig3.add_trace(go.Scatter(x=t, y=pg, mode=\"lines+markers\", name=\"ì„±ê³¼ê³„ì—´ KPI ì˜ˆì‹œ\"))\n",
        "    fig3.add_trace(go.Scatter(x=t, y=B,  mode=\"lines+markers\", name=\"ì¬ì •ê³„ì—´ KPI ì˜ˆì‹œ\"))\n",
        "    fig3.update_layout(title=\"KPI ì‹œë®¬ë ˆì´ì…˜(í‘œ í…ìŠ¤íŠ¸ ê¸°ì €ì„ )\", xaxis_title=\"ê¸°ê°„\",\n",
        "                       template=\"plotly_white\", height=360)\n",
        "    return fig1, fig2, fig3\n"
      ],
      "metadata": {
        "id": "IO5mNXrMXv0u"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def structure_score(text: str) -> float:\n",
        "    h = sum(1 for s in REQ_HEADS if s in text) / len(REQ_HEADS)\n",
        "    has_table = 1.0 if (\"|\" in text and \"---\" in text) else 0.0\n",
        "    has_math  = 1.0 if \"```math\" in text else 0.0\n",
        "    return round(0.6*h + 0.2*has_table + 0.2*has_math, 3)\n",
        "\n",
        "def novelty_score(candidate: str, baseline: str) -> float:\n",
        "    vec = TfidfVectorizer(min_df=1, max_df=0.95, ngram_range=(1,2))\n",
        "    X = vec.fit_transform([candidate, baseline])\n",
        "    sim = float(cosine_similarity(X[0], X[1])[0,0])\n",
        "    return round(max(0.0, 1.0 - sim), 3)\n",
        "\n",
        "def length_score(text: str) -> float:\n",
        "    n = len(text)\n",
        "    if n < 400:  return 0.4\n",
        "    if n > 8000: return 0.7\n",
        "    x = (min(2500, n) - 400) / (2500-400)\n",
        "    return round(0.6 + 0.4*max(0.0, min(1.0, x)), 3)\n",
        "\n",
        "@dataclass\n",
        "class RunConfig:\n",
        "    model_id: str\n",
        "    temperature: float\n",
        "    top_p: float\n",
        "\n",
        "def baseline_answer(q, model, temperature, top_p) -> str:\n",
        "    sys = BASELINE_SYSTEM\n",
        "    usr = BASELINE_USER_TPL.format(q=q)\n",
        "    return llm_long(sys, usr, model, temperature, top_p,\n",
        "                    segment_tokens=LONG_SEGMENT_TOKENS, max_segments=LONG_MAX_SEGMENTS)\n",
        "\n",
        "def deleuze_answer(q, ctx, model, temperature, top_p) -> str:\n",
        "    sys = DELEUZE_SYSTEM\n",
        "    usr = DELEUZE_USER_TPL.format(q=q, ctx=ctx or \"ì—†ìŒ\")\n",
        "    return llm_long(sys, usr, model, temperature, top_p,\n",
        "                    segment_tokens=LONG_SEGMENT_TOKENS, max_segments=LONG_MAX_SEGMENTS)\n",
        "\n",
        "def run_pipeline(q: str, ctx: str, cfg: RunConfig):\n",
        "    t0 = time.time()\n",
        "\n",
        "    # A) ìˆœì • GPT (ë¬¸ë‹¨í˜•)\n",
        "    baseline = baseline_answer(q, cfg.model_id, cfg.temperature, cfg.top_p)\n",
        "\n",
        "    # B) ë“¤ë¢°ì¦ˆ ì˜¤í¼ë ˆì´í„°(6ì„¹ì…˜ ê°•ì œ) + ëˆ„ë½ ë³´ì •\n",
        "    cand = deleuze_answer(q, ctx, cfg.model_id, cfg.temperature, cfg.top_p)\n",
        "    cand = repair_deleuze(cand, q, ctx, cfg.model_id, cfg.temperature, cfg.top_p)\n",
        "\n",
        "    # í”„ë¡ í‹°ì–´ í‘œ íŒŒì‹± â†’ í…ìŠ¤íŠ¸ ì§€ìˆ˜í™” â†’ ê·¸ë˜í”„\n",
        "    df_front = parse_markdown_table(cand)\n",
        "    auto_ok = df_front is not None\n",
        "    if not auto_ok:\n",
        "        # í‘œê°€ ì—†ì„ ê²½ìš° ìµœì†Œ í´ë°±(ì„ì˜ ìˆ˜ì¹˜ ì—†ì´ í…ìŠ¤íŠ¸ êµ¬ì¡°ë§Œ)\n",
        "        df_front = pd.DataFrame({\n",
        "            \"ëŒ€ì•ˆ\":     [\"ë³´ìˆ˜\",\"ê· í˜•\",\"ì§„ë³´\"],\n",
        "            \"í•µì‹¬ìˆ˜ë‹¨\": [\"ì„¸ìœ¨ ì™„ë§ŒÂ·ì •ë°€ í™˜ê¸‰Â·ì§‘í–‰ íš¨ìœ¨\", \"ê°€ê²©ì‹ í˜¸+í™˜ê¸‰ ê· í˜•Â·ì¤‘ê°„ ê°•ë„\", \"ê°•í•œ ê°€ê²©ì‹ í˜¸Â·ëŒ€ê·œëª¨ ì „í™˜ì§€ì›\"],\n",
        "            \"ê¸°ëŒ€íš¨ê³¼\": [\"ì ì§„ ê°œì„ \", \"ëª©í‘œ ê²½ë¡œ ê·¼ì ‘\", \"ê³ ê°•ë„ ì„±ê³¼\"],\n",
        "            \"Trade-off\":[\"ì†ë„Â·ë¶ˆí‰ë“± ë¦¬ìŠ¤í¬ ë‚®ìŒ\", \"ì¤‘ê°„ ë¶€ë‹´/ì„±ê³¼\", \"ì¬ì •Â·ìˆ˜ìš©ì„± ë¶€ë‹´\"],\n",
        "            \"ì¬ì •ì„¤ëª…\":[\"ì¬ì •ê±´ì „ì„± ìš°ì„ \", \"ê· í˜•ì  ì¬ì› êµ¬ì„±\", \"ì „í™˜ì§€ì› ì¤‘ì‹¬ ì¬ì›\"]\n",
        "        })\n",
        "    df_num = frontier_to_numeric(df_front)\n",
        "    fig1, fig2, fig3 = build_plots(df_num)\n",
        "\n",
        "    # ë­í‚¹ í…Œì´ë¸”\n",
        "    rank_df = df_num[[\"ëŒ€ì•ˆ\",\"ì„±ê³¼ ì§€ìˆ˜(í…ìŠ¤íŠ¸ ê¸°ë°˜)\",\"ì¬ì • ê¸°ì—¬ ì§€ìˆ˜(í…ìŠ¤íŠ¸ ê¸°ë°˜)\",\"ì¢…í•©ì ìˆ˜\"]].copy()\n",
        "    rank_df = rank_df.sort_values(\"ì¢…í•©ì ìˆ˜\", ascending=False).reset_index(drop=True)\n",
        "    rank_df.index = rank_df.index + 1\n",
        "    rank_df = rank_df.rename_axis(\"rank\").reset_index()\n",
        "\n",
        "    # ê°„ë‹¨ ì ìˆ˜(ì „ë¶€ ì˜¤í”ˆì†ŒìŠ¤)\n",
        "    s_struct = structure_score(cand)\n",
        "    s_novel  = novelty_score(cand, baseline)\n",
        "    s_len    = length_score(cand)\n",
        "    final_score = round(0.55*s_struct + 0.35*s_novel + 0.10*s_len, 3)\n",
        "\n",
        "    meta = {\n",
        "        \"runtime_sec\": round(time.time()-t0,2),\n",
        "        \"frontier_auto\": bool(auto_ok),\n",
        "        \"oss_modules\": [\"gradio\",\"plotly\",\"pandas\",\"scikit-learn\",\"regex\"],\n",
        "        \"scores\": {\"structure\": s_struct, \"novelty\": s_novel, \"length\": s_len, \"final\": final_score}\n",
        "    }\n",
        "    return baseline, cand, fig1, fig2, fig3, rank_df, meta\n"
      ],
      "metadata": {
        "id": "mKFNG-M8XvyW"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "try:\n",
        "    gr.close_all()\n",
        "except:\n",
        "    pass\n",
        "\n",
        "PORT = find_free_port()\n",
        "\n",
        "with gr.Blocks(title=\"A: ìˆœì • GPT vs B: GPT+ë“¤ë¢°ì¦ˆ â€” êµ¬ì¡°Â·í”„ë¡ í‹°ì–´Â·ìŠ¤ìœ„ì¹­Â·KPI\") as demo:\n",
        "    gr.Markdown(md(\"\"\"\n",
        "    ## ğŸ” ë¹„êµ ë°ëª¨ â€” **A: ìˆœì • GPT(ë¬¸ë‹¨)** vs **B: GPT+ë“¤ë¢°ì¦ˆ(í‘œÂ·ìˆ˜ì‹Â·ê·¸ë˜í”„Â·ë­í‚¹)**\n",
        "    - ìƒì„±ì€ OpenAI API. í‘œ íŒŒì‹±/ì§€ìˆ˜í™”/ê·¸ë˜í”„/ë­í‚¹/í‰ê°€ëŠ” **ëª¨ë‘ ì˜¤í”ˆì†ŒìŠ¤**ë¡œ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
        "    - í”„ë¡ í‹°ì–´ ì¶•(ì„±ê³¼/ì¬ì •)ì€ **í‘œì˜ í…ìŠ¤íŠ¸ ê·¼ê±°(í‚¤ì›Œë“œÂ·ìˆ«ì)** ë¥¼ ì •ê·œí™”í•´ ì‚°ì¶œí•©ë‹ˆë‹¤(ì„ì˜ ìˆ˜ì¹˜ ì‚¬ìš© ì—†ìŒ).\n",
        "    \"\"\"))\n",
        "\n",
        "    with gr.Accordion(\"ğŸ”‘ OpenAI API í‚¤\", open=True):\n",
        "        key = gr.Textbox(type=\"password\", placeholder=\"sk-...\")\n",
        "        out = gr.Markdown()\n",
        "        def set_key(k):\n",
        "            os.environ[\"OPENAI_API_KEY\"]=k.strip()\n",
        "            return \"âœ… ì €ì¥ ì™„ë£Œ\"\n",
        "        gr.Button(\"í‚¤ ì €ì¥\").click(set_key, inputs=[key], outputs=[out])\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1):\n",
        "            q   = gr.Textbox(label=\"ì§ˆë¬¸\", lines=6,\n",
        "                             placeholder=\"ì˜ˆ) íƒ„ì†Œì„¸ì™€ ì „í™˜ì§€ì›ìœ¼ë¡œ 2030ë…„ â€“40% ê°ì¶• ì„¤ê³„â€¦\")\n",
        "            ctx = gr.Textbox(label=\"ê·¼ê±° í…ìŠ¤íŠ¸(ì„ íƒ)\", lines=6,\n",
        "                             placeholder=\"í”„ë¡ í‹°ì–´ í‘œ ì»¬ëŸ¼, KPI, ì„ê³„ì¹˜(Î¸), ì¡°ì •í­(Î´) ë“± íŒíŠ¸ë¥¼ ë„£ìœ¼ë©´ êµ¬ì¡°ê°€ ë” ì •ë°€í•´ì§‘ë‹ˆë‹¤.\")\n",
        "            with gr.Accordion(\"ê³ ê¸‰ì„¤ì •\", open=False):\n",
        "                model = gr.Textbox(value=\"gpt-4o-mini\", label=\"OpenAI Model ID\")\n",
        "                temp  = gr.Slider(0.0, 1.2, value=0.7, step=0.05, label=\"temperature\")\n",
        "                top_p = gr.Slider(0.1, 1.0, value=0.9, step=0.05, label=\"top_p\")\n",
        "            run_btn = gr.Button(\"ì‹¤í–‰\", variant=\"primary\")\n",
        "            meta_box = gr.Markdown()\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1):\n",
        "            base_box = gr.Markdown()  # ë¬¸ë‹¨í˜• ê°€ë…ì„± ìœ„í•´ Markdown ì‚¬ìš©\n",
        "        with gr.Column(scale=2):\n",
        "            deleuze_box = gr.Markdown()  # í‘œ/ìˆ˜ì‹ ë Œë”ë§ì„ ìœ„í•´ Markdown\n",
        "            with gr.Row():\n",
        "                fig1 = gr.Plot(label=\"í”„ë¡ í‹°ì–´ â€” ì„±ê³¼ vs ì¬ì •(í‘œ í…ìŠ¤íŠ¸ ê·¼ê±°)\")\n",
        "                fig2 = gr.Plot(label=\"í•µì‹¬ íŒŒë¼ë¯¸í„° ë¹„êµ(í…ìŠ¤íŠ¸ ìœ ë„)\")\n",
        "            fig3 = gr.Plot(label=\"KPI ì‹œë®¬ë ˆì´ì…˜(í‘œ í…ìŠ¤íŠ¸ ê¸°ì €ì„ )\")\n",
        "            rank_tbl = gr.Dataframe(label=\"ëŒ€ì•ˆ ë­í‚¹(í…ìŠ¤íŠ¸ ê¸°ë°˜ ì§€ìˆ˜)\", wrap=True, interactive=False)\n",
        "\n",
        "    def on_run(question, context, model_id, temperature, top_p):\n",
        "        cfg = RunConfig(model_id=model_id, temperature=float(temperature), top_p=float(top_p))\n",
        "        try:\n",
        "            b, d, f1, f2, f3, rank_df, meta = run_pipeline(question, context, cfg)\n",
        "            sc = meta[\"scores\"]\n",
        "            chip = (\n",
        "                f\"**Runtime**: {meta['runtime_sec']}s Â· \"\n",
        "                f\"**Frontier**: {'í‘œ ìë™íŒŒì‹±âœ…' if meta['frontier_auto'] else 'í‘œ ë¶€ì¬â†’í…ìŠ¤íŠ¸ í´ë°±âš ï¸'}  \\n\"\n",
        "                f\"**ì¶• ì •ì˜**: ì„±ê³¼/ì¬ì • ì§€ìˆ˜ëŠ” **í‘œ í…ìŠ¤íŠ¸ ê·¼ê±°**ë¡œ ì •ê·œí™” ì‚°ì¶œ  \\n\"\n",
        "                f\"**OSS**: gradio, plotly, pandas, scikit-learn, regex  \\n\"\n",
        "                f\"**(ì°¸ê³ )** êµ¬ì¡° {sc['structure']}, ì°½ì˜ {sc['novelty']}, ê¸¸ì´ {sc['length']}, ìµœì¢… {sc['final']}\"\n",
        "            )\n",
        "            # A/Bë¥¼ Markdownìœ¼ë¡œ ë…¸ì¶œ (AëŠ” ë¬¸ë‹¨, BëŠ” ì„¹ì…˜/í‘œ/ìˆ˜ì‹)\n",
        "            base_md    = md(b)\n",
        "            deleuze_md = md(d)\n",
        "            return base_md, deleuze_md, f1, f2, f3, rank_df, chip\n",
        "        except Exception as e:\n",
        "            tb = \"```\\n\"+traceback.format_exc()+\"\\n```\"\n",
        "            return \"âŒ ì˜¤ë¥˜\", f\"âŒ ì˜¤ë¥˜: {e}\\n{tb}\", go.Figure(), go.Figure(), go.Figure(), pd.DataFrame(), \"\"\n",
        "\n",
        "    run_btn.click(on_run, inputs=[q, ctx, model, temp, top_p],\n",
        "                  outputs=[base_box, deleuze_box, fig1, fig2, fig3, rank_tbl, meta_box])\n",
        "\n",
        "demo.queue(max_size=32).launch(share=True, server_name=\"0.0.0.0\", server_port=PORT, show_error=True)\n",
        "print(\"ğŸ”— Port:\", PORT)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 695
        },
        "id": "RJzYYO2QXvv1",
        "outputId": "7a70c3e1-3e36-4e90-c360-213c222942cc"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/gradio/analytics.py:106: UserWarning:\n",
            "\n",
            "IMPORTANT: You are using gradio version 4.44.0, however version 4.44.1 is available, please upgrade. \n",
            "--------\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on public URL: https://e5ed36aa772b4aa1d8.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://e5ed36aa772b4aa1d8.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”— Port: 7862\n"
          ]
        }
      ]
    }
  ]
}